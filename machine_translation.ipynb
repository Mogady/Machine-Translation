{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "In this notebook, i will build a deep neural network that functions as part of an end-to-end machine translation pipeline.completed pipeline will accept English text as input and return the French translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  i'll be using a dataset udacity ai  created for this project that contains a small vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize \n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    data = tokenizer.texts_to_sequences(x)\n",
    "    return (data,tokenizer)\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "\n",
    "    if not length:\n",
    "        m=len(max(x, key=len))\n",
    "        pad=pad_sequences(x,maxlen=m,padding='post')\n",
    "    else:\n",
    "        pad=pad_sequences(x,maxlen=length,padding='post')\n",
    "    return pad\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, i will experiment with various neural network architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN \n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_33 (GRU)                 (None, 21, 21)            1449      \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 21, 688)           15136     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 253,601\n",
      "Trainable params: 253,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_34 (GRU)                 (None, 21, 21)            1449      \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 21, 688)           15136     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 253,601\n",
      "Trainable params: 253,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 10s 87us/step - loss: 3.2631 - acc: 0.4479 - val_loss: nan - val_acc: 0.4880\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 8s 76us/step - loss: 2.2395 - acc: 0.5086 - val_loss: nan - val_acc: 0.5393\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 8s 74us/step - loss: 1.9174 - acc: 0.5597 - val_loss: nan - val_acc: 0.5742\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 8s 71us/step - loss: 1.6631 - acc: 0.5856 - val_loss: nan - val_acc: 0.5892\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 8s 75us/step - loss: 1.5390 - acc: 0.5974 - val_loss: nan - val_acc: 0.6041\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 8s 75us/step - loss: 1.4652 - acc: 0.6081 - val_loss: nan - val_acc: 0.6075\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 8s 76us/step - loss: 1.4108 - acc: 0.6161 - val_loss: nan - val_acc: 0.6232\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 8s 77us/step - loss: 1.3706 - acc: 0.6225 - val_loss: nan - val_acc: 0.6289\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 8s 76us/step - loss: 1.3387 - acc: 0.6280 - val_loss: nan - val_acc: 0.6310\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 8s 76us/step - loss: 1.3102 - acc: 0.6334 - val_loss: nan - val_acc: 0.6350\n",
      "new jersey est parfois parfois en en et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate=0.001\n",
    "    inputs=Input(shape=input_shape[1:])\n",
    "    gru=GRU(units=output_sequence_length,return_sequences=True)(inputs)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, \n",
    "                                    activation='relu'))(gru)\n",
    "    outputs = Dense(french_vocab_size, \n",
    "                                    activation='softmax')(layers)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                 metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding \n",
    "![RNN](images/embedding.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 21, 199)           39601     \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 (None, 21, 21)            13923     \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 21, 688)           15136     \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 305,676\n",
      "Trainable params: 305,676\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 21, 200)           40000     \n",
      "_________________________________________________________________\n",
      "gru_27 (GRU)                 (None, 21, 21)            13986     \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 21, 690)           15180     \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 21, 345)           238395    \n",
      "=================================================================\n",
      "Total params: 307,561\n",
      "Trainable params: 307,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 9s 84us/step - loss: 2.9623 - acc: 0.4850 - val_loss: 1.6507 - val_acc: 0.6005\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 8s 71us/step - loss: 1.2796 - acc: 0.6629 - val_loss: 1.0406 - val_acc: 0.6993\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 8s 71us/step - loss: 0.9298 - acc: 0.7187 - val_loss: 0.8347 - val_acc: 0.7392\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 8s 70us/step - loss: 0.7621 - acc: 0.7616 - val_loss: 0.6913 - val_acc: 0.7856\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 7s 68us/step - loss: 0.6493 - acc: 0.7973 - val_loss: 0.6093 - val_acc: 0.8094\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 8s 68us/step - loss: 0.5826 - acc: 0.8163 - val_loss: 0.5537 - val_acc: 0.8241\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.5343 - acc: 0.8308 - val_loss: 0.5141 - val_acc: 0.8375\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 7s 68us/step - loss: 0.4970 - acc: 0.8412 - val_loss: 0.4806 - val_acc: 0.8464\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.4672 - acc: 0.8504 - val_loss: 0.4527 - val_acc: 0.8551\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 7s 67us/step - loss: 0.4416 - acc: 0.8585 - val_loss: 0.4292 - val_acc: 0.8621\n",
      "new jersey est parfois calme en l' et il il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate=0.001\n",
    "    inputs=Input(shape=input_shape[1:])\n",
    "    encoded_inputs=layers = Embedding(english_vocab_size, english_vocab_size)(inputs)\n",
    "    gru=GRU(units=output_sequence_length,return_sequences=True)(encoded_inputs)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, \n",
    "                                    activation='relu'))(gru)\n",
    "    outputs = Dense(french_vocab_size, \n",
    "                                    activation='softmax')(layers)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                 metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, \n",
    "                    epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs \n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 21, 42)            2898      \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 21, 690)           29670     \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 21, 345)           238395    \n",
      "=================================================================\n",
      "Total params: 270,963\n",
      "Trainable params: 270,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 10s 93us/step - loss: 3.0540 - acc: 0.4821 - val_loss: 2.0222 - val_acc: 0.5497\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 8s 74us/step - loss: 1.7707 - acc: 0.5797 - val_loss: 1.5976 - val_acc: 0.5961\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 1.5154 - acc: 0.6038 - val_loss: 1.4505 - val_acc: 0.6132\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 1.4070 - acc: 0.6195 - val_loss: 1.3674 - val_acc: 0.6284\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 9s 81us/step - loss: 1.3351 - acc: 0.6329 - val_loss: 1.3022 - val_acc: 0.6384\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 9s 85us/step - loss: 1.2771 - acc: 0.6415 - val_loss: 1.2533 - val_acc: 0.6456\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 10s 87us/step - loss: 1.2326 - acc: 0.6478 - val_loss: 1.2133 - val_acc: 0.6542\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 10s 87us/step - loss: 1.1960 - acc: 0.6545 - val_loss: 1.1790 - val_acc: 0.6584\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 9s 85us/step - loss: 1.1647 - acc: 0.6603 - val_loss: 1.1497 - val_acc: 0.6640\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 9s 83us/step - loss: 1.1364 - acc: 0.6649 - val_loss: 1.1244 - val_acc: 0.6668\n",
      "new jersey est parfois calme en en et il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate=0.001\n",
    "    inputs=Input(shape=input_shape[1:])\n",
    "    gru=Bidirectional(GRU(units=output_sequence_length,return_sequences=True))(inputs)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, \n",
    "                                    activation='relu'))(gru)\n",
    "    outputs =Dense(french_vocab_size, \n",
    "                                    activation='softmax')(layers)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                 metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)\n",
    "\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, \n",
    "                    epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder \n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        (None, 15, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_37 (GRU)                 (None, 21)                1449      \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 21)            0         \n",
      "_________________________________________________________________\n",
      "gru_38 (GRU)                 (None, 21, 21)            2709      \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 21, 688)           15136     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 256,310\n",
      "Trainable params: 256,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_39 (GRU)                 (None, 21)                1449      \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 21, 21)            0         \n",
      "_________________________________________________________________\n",
      "gru_40 (GRU)                 (None, 21, 21)            2709      \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 21, 690)           15180     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 21, 345)           238395    \n",
      "=================================================================\n",
      "Total params: 257,733\n",
      "Trainable params: 257,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 12s 109us/step - loss: 3.7343 - acc: 0.4044 - val_loss: 2.9111 - val_acc: 0.4093\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 9s 85us/step - loss: 2.5744 - acc: 0.4604 - val_loss: 2.3290 - val_acc: 0.4926\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 9s 86us/step - loss: 2.2195 - acc: 0.5028 - val_loss: 2.1236 - val_acc: 0.5108\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 10s 88us/step - loss: 2.0516 - acc: 0.5245 - val_loss: 1.9872 - val_acc: 0.5363\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 10s 86us/step - loss: 1.9469 - acc: 0.5373 - val_loss: 1.9091 - val_acc: 0.5392\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 10s 92us/step - loss: 1.8888 - acc: 0.5424 - val_loss: 1.8646 - val_acc: 0.5453\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 10s 90us/step - loss: 1.8415 - acc: 0.5480 - val_loss: 1.8083 - val_acc: 0.5548\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.7287 - acc: 0.5623 - val_loss: 1.6479 - val_acc: 0.5703\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 10s 93us/step - loss: 1.6215 - acc: 0.5743 - val_loss: 1.6028 - val_acc: 0.5765\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 10s 94us/step - loss: 1.5815 - acc: 0.5812 - val_loss: 1.5654 - val_acc: 0.5862\n",
      "new jersey est jamais en en et il il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    learning_rate = .001\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    gru = GRU(output_sequence_length)(inputs)\n",
    "    repeats = RepeatVector(output_sequence_length)(gru)\n",
    "    layers = GRU(output_sequence_length,return_sequences=True)(repeats)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, \n",
    "                                    activation='relu'))(layers)\n",
    "    outputs =Dense(french_vocab_size, \n",
    "                                    activation='softmax')(layers)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        optimizer=Adam(learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, \n",
    "                    epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom\n",
    "here i will make a custom model from all the previous ones to try get better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_56 (InputLayer)        (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_33 (Embedding)     (None, 15, 199)           39601     \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 42)                27846     \n",
      "_________________________________________________________________\n",
      "repeat_vector_19 (RepeatVect (None, 21, 42)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 21, 42)            8064      \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 21, 688)           29584     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 342,111\n",
      "Trainable params: 342,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "\n",
    "    # build the layers\n",
    "    learning_rate = .005\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    layers = Embedding(english_vocab_size, english_vocab_size, \n",
    "                       mask_zero=False)(inputs)\n",
    "    layers = Bidirectional(GRU(output_sequence_length))(layers)\n",
    "    layers = RepeatVector(output_sequence_length)(layers)\n",
    "    layers = Bidirectional(GRU(output_sequence_length,return_sequences=True))(layers)\n",
    "    layers = TimeDistributed(Dense(2 * french_vocab_size, activation='relu'))(layers)\n",
    "    outputs = Dense(french_vocab_size, activation='softmax')(layers)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=sparse_categorical_crossentropy,\n",
    "        optimizer=Adam(learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "tests.test_model_final(model_final)\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue with last save: final_model.h5\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.8777 - acc: 0.5209Epoch 00001: val_loss improved from inf to 1.57756, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 33s 295us/step - loss: 1.8766 - acc: 0.5211 - val_loss: 1.5776 - val_acc: 0.5930\n",
      "Epoch 2/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.6090 - acc: 0.5752- ETA: 5s -Epoch 00002: val_loss improved from 1.57756 to 1.32298, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 1.6079 - acc: 0.5754 - val_loss: 1.3230 - val_acc: 0.6491\n",
      "Epoch 3/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.4395 - acc: 0.6070Epoch 00003: val_loss improved from 1.32298 to 1.21777, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 1.4389 - acc: 0.6072 - val_loss: 1.2178 - val_acc: 0.6680\n",
      "Epoch 4/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.3141 - acc: 0.6340- ETA: 5s - Epoch 00004: val_loss improved from 1.21777 to 1.19334, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 1.3137 - acc: 0.6341 - val_loss: 1.1933 - val_acc: 0.6534\n",
      "Epoch 5/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.2215 - acc: 0.6559Epoch 00005: val_loss improved from 1.19334 to 1.06581, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 1.2210 - acc: 0.6560 - val_loss: 1.0658 - val_acc: 0.6987\n",
      "Epoch 6/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1808 - acc: 0.6651Epoch 00006: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 1.1812 - acc: 0.6650 - val_loss: 1.1216 - val_acc: 0.6794\n",
      "Epoch 7/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1333 - acc: 0.6761Epoch 00007: val_loss improved from 1.06581 to 1.02971, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 1.1334 - acc: 0.6761 - val_loss: 1.0297 - val_acc: 0.7055\n",
      "Epoch 8/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0955 - acc: 0.6849Epoch 00008: val_loss improved from 1.02971 to 0.96883, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 1.0955 - acc: 0.6849 - val_loss: 0.9688 - val_acc: 0.7215\n",
      "Epoch 9/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0687 - acc: 0.6914Epoch 00009: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 1.0686 - acc: 0.6914 - val_loss: 1.0017 - val_acc: 0.7121\n",
      "Epoch 10/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0351 - acc: 0.6997Epoch 00010: val_loss improved from 0.96883 to 0.91016, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 1.0347 - acc: 0.6998 - val_loss: 0.9102 - val_acc: 0.7350\n",
      "Epoch 11/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0025 - acc: 0.7072Epoch 00011: val_loss improved from 0.91016 to 0.88839, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 1.0029 - acc: 0.7071 - val_loss: 0.8884 - val_acc: 0.7377\n",
      "Epoch 12/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9727 - acc: 0.7136Epoch 00012: val_loss improved from 0.88839 to 0.86831, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.9725 - acc: 0.7136 - val_loss: 0.8683 - val_acc: 0.7442\n",
      "Epoch 13/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9543 - acc: 0.7170Epoch 00013: val_loss improved from 0.86831 to 0.80133, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.9539 - acc: 0.7171 - val_loss: 0.8013 - val_acc: 0.7606\n",
      "Epoch 14/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9234 - acc: 0.7239Epoch 00014: val_loss improved from 0.80133 to 0.78141, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.9233 - acc: 0.7239 - val_loss: 0.7814 - val_acc: 0.7645\n",
      "Epoch 15/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9151 - acc: 0.7244Epoch 00015: val_loss improved from 0.78141 to 0.75971, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.9150 - acc: 0.7244 - val_loss: 0.7597 - val_acc: 0.7678\n",
      "Epoch 16/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8717 - acc: 0.7347Epoch 00016: val_loss improved from 0.75971 to 0.72551, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.8718 - acc: 0.7347 - val_loss: 0.7255 - val_acc: 0.7777\n",
      "Epoch 17/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8553 - acc: 0.7382Epoch 00017: val_loss improved from 0.72551 to 0.71732, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.8552 - acc: 0.7382 - val_loss: 0.7173 - val_acc: 0.7801\n",
      "Epoch 18/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8518 - acc: 0.7381Epoch 00018: val_loss improved from 0.71732 to 0.69581, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.8515 - acc: 0.7382 - val_loss: 0.6958 - val_acc: 0.7859\n",
      "Epoch 19/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7465Epoch 00019: val_loss improved from 0.69581 to 0.67565, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.8171 - acc: 0.7465 - val_loss: 0.6756 - val_acc: 0.7898\n",
      "Epoch 20/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7470Epoch 00020: val_loss improved from 0.67565 to 0.66929, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.8115 - acc: 0.7469 - val_loss: 0.6693 - val_acc: 0.7897\n",
      "Epoch 21/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7549Epoch 00021: val_loss improved from 0.66929 to 0.64012, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.7803 - acc: 0.7550 - val_loss: 0.6401 - val_acc: 0.7975\n",
      "Epoch 22/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7798 - acc: 0.7546Epoch 00022: val_loss improved from 0.64012 to 0.62783, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.7797 - acc: 0.7546 - val_loss: 0.6278 - val_acc: 0.8023\n",
      "Epoch 23/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7594 - acc: 0.7596Epoch 00023: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.7594 - acc: 0.7596 - val_loss: 0.6438 - val_acc: 0.7937\n",
      "Epoch 24/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7542 - acc: 0.7607Epoch 00024: val_loss improved from 0.62783 to 0.60805, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.7539 - acc: 0.7607 - val_loss: 0.6081 - val_acc: 0.8064\n",
      "Epoch 25/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7330 - acc: 0.7660Epoch 00025: val_loss improved from 0.60805 to 0.58884, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.7329 - acc: 0.7660 - val_loss: 0.5888 - val_acc: 0.8124\n",
      "Epoch 26/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.7672Epoch 00026: val_loss improved from 0.58884 to 0.58113, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.7247 - acc: 0.7672 - val_loss: 0.5811 - val_acc: 0.8153\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7029 - acc: 0.7732Epoch 00027: val_loss improved from 0.58113 to 0.56458, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.7030 - acc: 0.7732 - val_loss: 0.5646 - val_acc: 0.8186\n",
      "Epoch 28/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6985 - acc: 0.7743Epoch 00028: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6990 - acc: 0.7742 - val_loss: 0.6328 - val_acc: 0.7931\n",
      "Epoch 29/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.7757Epoch 00029: val_loss improved from 0.56458 to 0.53941, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6911 - acc: 0.7758 - val_loss: 0.5394 - val_acc: 0.8243\n",
      "Epoch 30/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.7807- ETA: Epoch 00030: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6731 - acc: 0.7808 - val_loss: 0.5444 - val_acc: 0.8231\n",
      "Epoch 31/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6678 - acc: 0.7822Epoch 00031: val_loss improved from 0.53941 to 0.52696, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6677 - acc: 0.7822 - val_loss: 0.5270 - val_acc: 0.8259\n",
      "Epoch 32/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7838Epoch 00032: val_loss improved from 0.52696 to 0.52102, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6621 - acc: 0.7839 - val_loss: 0.5210 - val_acc: 0.8296\n",
      "Epoch 33/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6450 - acc: 0.7889Epoch 00033: val_loss improved from 0.52102 to 0.50642, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.6449 - acc: 0.7890 - val_loss: 0.5064 - val_acc: 0.8326\n",
      "Epoch 34/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6516 - acc: 0.7865Epoch 00034: val_loss improved from 0.50642 to 0.49814, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6514 - acc: 0.7866 - val_loss: 0.4981 - val_acc: 0.8363\n",
      "Epoch 35/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.7935- ETA: 7s - loss: 0.6279 - acc: 0 - ETA: 6Epoch 00035: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.6271 - acc: 0.7935 - val_loss: 0.5066 - val_acc: 0.8333\n",
      "Epoch 36/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.7938Epoch 00036: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.6249 - acc: 0.7936 - val_loss: 0.6529 - val_acc: 0.7835\n",
      "Epoch 37/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.7940Epoch 00037: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.6245 - acc: 0.7940 - val_loss: 0.5248 - val_acc: 0.8271\n",
      "Epoch 38/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6306 - acc: 0.7918Epoch 00038: val_loss improved from 0.49814 to 0.47932, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.6306 - acc: 0.7918 - val_loss: 0.4793 - val_acc: 0.8408\n",
      "Epoch 39/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.8013Epoch 00039: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5990 - acc: 0.8013 - val_loss: 0.4836 - val_acc: 0.8391\n",
      "Epoch 40/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8018- ETA:Epoch 00040: val_loss improved from 0.47932 to 0.47074, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 222us/step - loss: 0.5956 - acc: 0.8018 - val_loss: 0.4707 - val_acc: 0.8428\n",
      "Epoch 41/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5880 - acc: 0.8046- ETA: 4s - losEpoch 00041: val_loss improved from 0.47074 to 0.46322, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 222us/step - loss: 0.5881 - acc: 0.8046 - val_loss: 0.4632 - val_acc: 0.8443\n",
      "Epoch 42/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.8034Epoch 00042: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 222us/step - loss: 0.5921 - acc: 0.8030 - val_loss: 1.0439 - val_acc: 0.6751\n",
      "Epoch 43/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.8000Epoch 00043: val_loss improved from 0.46322 to 0.45354, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 224us/step - loss: 0.6050 - acc: 0.8000 - val_loss: 0.4535 - val_acc: 0.8476\n",
      "Epoch 44/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.8093Epoch 00044: val_loss improved from 0.45354 to 0.44653, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.5711 - acc: 0.8093 - val_loss: 0.4465 - val_acc: 0.8495\n",
      "Epoch 45/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.8103Epoch 00045: val_loss did not improve\n",
      "110288/110288 [==============================] - 25s 223us/step - loss: 0.5674 - acc: 0.8102 - val_loss: 0.5310 - val_acc: 0.8244\n",
      "Epoch 46/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.7924- ETA: 1s - loss: 0.6383 - acc: Epoch 00046: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.6333 - acc: 0.7926 - val_loss: 0.4470 - val_acc: 0.8518\n",
      "Epoch 47/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8101Epoch 00047: val_loss improved from 0.44653 to 0.44108, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 226us/step - loss: 0.5685 - acc: 0.8100 - val_loss: 0.4411 - val_acc: 0.8546\n",
      "Epoch 48/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5606 - acc: 0.8124- ETA: 1s - loss: 0.5621 - acc:Epoch 00048: val_loss improved from 0.44108 to 0.43386, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 222us/step - loss: 0.5605 - acc: 0.8124 - val_loss: 0.4339 - val_acc: 0.8525\n",
      "Epoch 49/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5515 - acc: 0.8148Epoch 00049: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.5516 - acc: 0.8148 - val_loss: 0.4445 - val_acc: 0.8521\n",
      "Epoch 50/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8164Epoch 00050: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.5466 - acc: 0.8164 - val_loss: 0.4396 - val_acc: 0.8533\n",
      "Epoch 51/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8189Epoch 00051: val_loss improved from 0.43386 to 0.42617, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5396 - acc: 0.8188 - val_loss: 0.4262 - val_acc: 0.8564\n",
      "Epoch 52/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.8067Epoch 00052: val_loss improved from 0.42617 to 0.42560, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5828 - acc: 0.8067 - val_loss: 0.4256 - val_acc: 0.8572\n",
      "Epoch 53/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8198Epoch 00053: val_loss did not improve\n",
      "110288/110288 [==============================] - 25s 224us/step - loss: 0.5367 - acc: 0.8198 - val_loss: 0.4313 - val_acc: 0.8571\n",
      "Epoch 54/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5317 - acc: 0.8211Epoch 00054: val_loss improved from 0.42560 to 0.41603, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 226us/step - loss: 0.5317 - acc: 0.8211 - val_loss: 0.4160 - val_acc: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8212Epoch 00055: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5331 - acc: 0.8212 - val_loss: 0.4176 - val_acc: 0.8625\n",
      "Epoch 56/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.8224Epoch 00056: val_loss improved from 0.41603 to 0.41049, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.5286 - acc: 0.8225 - val_loss: 0.4105 - val_acc: 0.8610\n",
      "Epoch 57/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8103  ETA: 19s - loEpoch 00057: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 0.5739 - acc: 0.8104 - val_loss: 0.4158 - val_acc: 0.8624\n",
      "Epoch 58/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8246Epoch 00058: val_loss did not improve\n",
      "110288/110288 [==============================] - 25s 226us/step - loss: 0.5221 - acc: 0.8247 - val_loss: 0.4188 - val_acc: 0.8619\n",
      "Epoch 59/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8246Epoch 00059: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 221us/step - loss: 0.5212 - acc: 0.8246 - val_loss: 0.4107 - val_acc: 0.8621\n",
      "Epoch 60/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8287Epoch 00060: val_loss improved from 0.41049 to 0.39967, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5078 - acc: 0.8288 - val_loss: 0.3997 - val_acc: 0.8656\n",
      "Epoch 61/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8268- ETA: 1s - loss: 0.5152 - acc: Epoch 00061: val_loss improved from 0.39967 to 0.39841, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5149 - acc: 0.8268 - val_loss: 0.3984 - val_acc: 0.8653\n",
      "Epoch 62/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5061 - acc: 0.8294Epoch 00062: val_loss improved from 0.39841 to 0.39608, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 0.5060 - acc: 0.8294 - val_loss: 0.3961 - val_acc: 0.8666\n",
      "Epoch 63/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8286Epoch 00063: val_loss improved from 0.39608 to 0.39458, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.5085 - acc: 0.8286 - val_loss: 0.3946 - val_acc: 0.8669\n",
      "Epoch 64/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8284Epoch 00064: val_loss improved from 0.39458 to 0.38737, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.5115 - acc: 0.8285 - val_loss: 0.3874 - val_acc: 0.8709\n",
      "Epoch 65/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5656 - acc: 0.8135Epoch 00065: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 220us/step - loss: 0.5655 - acc: 0.8135 - val_loss: 0.4169 - val_acc: 0.8619\n",
      "Epoch 66/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8319Epoch 00066: val_loss improved from 0.38737 to 0.38688, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 224us/step - loss: 0.4989 - acc: 0.8319 - val_loss: 0.3869 - val_acc: 0.8672\n",
      "Epoch 67/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8314Epoch 00067: val_loss improved from 0.38688 to 0.38130, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.5002 - acc: 0.8314 - val_loss: 0.3813 - val_acc: 0.8721\n",
      "Epoch 68/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8350Epoch 00068: val_loss improved from 0.38130 to 0.37868, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 0.4879 - acc: 0.8350 - val_loss: 0.3787 - val_acc: 0.8718\n",
      "Epoch 69/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.8345Epoch 00069: val_loss improved from 0.37868 to 0.37355, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4887 - acc: 0.8345 - val_loss: 0.3736 - val_acc: 0.8746\n",
      "Epoch 70/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8376Epoch 00070: val_loss improved from 0.37355 to 0.37282, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.4805 - acc: 0.8376 - val_loss: 0.3728 - val_acc: 0.8753\n",
      "Epoch 71/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.8377Epoch 00071: val_loss improved from 0.37282 to 0.37093, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 226us/step - loss: 0.4779 - acc: 0.8377 - val_loss: 0.3709 - val_acc: 0.8729\n",
      "Epoch 72/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8391Epoch 00072: val_loss improved from 0.37093 to 0.35835, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.4740 - acc: 0.8392 - val_loss: 0.3584 - val_acc: 0.8782\n",
      "Epoch 73/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4951 - acc: 0.8326Epoch 00073: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.4949 - acc: 0.8327 - val_loss: 0.3592 - val_acc: 0.8793\n",
      "Epoch 74/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8427Epoch 00074: val_loss improved from 0.35835 to 0.35244, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 221us/step - loss: 0.4629 - acc: 0.8427 - val_loss: 0.3524 - val_acc: 0.8822\n",
      "Epoch 75/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8447Epoch 00075: val_loss improved from 0.35244 to 0.35114, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 25s 225us/step - loss: 0.4572 - acc: 0.8447 - val_loss: 0.3511 - val_acc: 0.8818\n",
      "Epoch 76/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4534 - acc: 0.8456Epoch 00076: val_loss improved from 0.35114 to 0.34622, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 218us/step - loss: 0.4533 - acc: 0.8456 - val_loss: 0.3462 - val_acc: 0.8835\n",
      "Epoch 77/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8432Epoch 00077: val_loss improved from 0.34622 to 0.33914, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4617 - acc: 0.8432 - val_loss: 0.3391 - val_acc: 0.8858\n",
      "Epoch 78/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8485Epoch 00078: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.4453 - acc: 0.8485 - val_loss: 0.3463 - val_acc: 0.8834\n",
      "Epoch 79/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8479Epoch 00079: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.4461 - acc: 0.8479 - val_loss: 0.3397 - val_acc: 0.8852\n",
      "Epoch 80/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.8247Epoch 00080: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 0.5392 - acc: 0.8248 - val_loss: 0.3454 - val_acc: 0.8834\n",
      "Epoch 81/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8504Epoch 00081: val_loss improved from 0.33914 to 0.33499, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 222us/step - loss: 0.4398 - acc: 0.8504 - val_loss: 0.3350 - val_acc: 0.8861\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4358 - acc: 0.8513Epoch 00082: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 217us/step - loss: 0.4360 - acc: 0.8512 - val_loss: 0.3390 - val_acc: 0.8850\n",
      "Epoch 83/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8508Epoch 00083: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4373 - acc: 0.8509 - val_loss: 0.3360 - val_acc: 0.8869\n",
      "Epoch 84/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8513Epoch 00084: val_loss improved from 0.33499 to 0.32899, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4355 - acc: 0.8512 - val_loss: 0.3290 - val_acc: 0.8872\n",
      "Epoch 85/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8519Epoch 00085: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4336 - acc: 0.8519 - val_loss: 0.3326 - val_acc: 0.8876\n",
      "Epoch 86/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8529Epoch 00086: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4303 - acc: 0.8530 - val_loss: 0.3413 - val_acc: 0.8853\n",
      "Epoch 87/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.8535Epoch 00087: val_loss improved from 0.32899 to 0.32505, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4296 - acc: 0.8535 - val_loss: 0.3251 - val_acc: 0.8904\n",
      "Epoch 88/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8365Epoch 00088: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4915 - acc: 0.8366 - val_loss: 0.3302 - val_acc: 0.8888\n",
      "Epoch 89/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8553Epoch 00089: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4235 - acc: 0.8554 - val_loss: 0.3385 - val_acc: 0.8863\n",
      "Epoch 90/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8558Epoch 00090: val_loss improved from 0.32505 to 0.32395, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4218 - acc: 0.8559 - val_loss: 0.3239 - val_acc: 0.8900\n",
      "Epoch 91/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8560Epoch 00091: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4205 - acc: 0.8561 - val_loss: 0.3248 - val_acc: 0.8891\n",
      "Epoch 92/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8551Epoch 00092: val_loss improved from 0.32395 to 0.32103, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 216us/step - loss: 0.4235 - acc: 0.8551 - val_loss: 0.3210 - val_acc: 0.8905\n",
      "Epoch 93/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.8562- ETA: 2s - loss: 0.4205Epoch 00093: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 215us/step - loss: 0.4209 - acc: 0.8562 - val_loss: 0.3240 - val_acc: 0.8893\n",
      "Epoch 94/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8537Epoch 00094: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4272 - acc: 0.8538 - val_loss: 0.3281 - val_acc: 0.8886\n",
      "Epoch 95/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8565  ETA: 10s - loss: - ETA: 5sEpoch 00095: val_loss improved from 0.32103 to 0.31675, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4189 - acc: 0.8565 - val_loss: 0.3168 - val_acc: 0.8918\n",
      "Epoch 96/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8578Epoch 00096: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4148 - acc: 0.8578 - val_loss: 0.3218 - val_acc: 0.8910\n",
      "Epoch 97/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8547Epoch 00097: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4258 - acc: 0.8547 - val_loss: 0.3256 - val_acc: 0.8880\n",
      "Epoch 98/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8594Epoch 00098: val_loss improved from 0.31675 to 0.31596, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4103 - acc: 0.8594 - val_loss: 0.3160 - val_acc: 0.8923\n",
      "Epoch 99/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8597Epoch 00099: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s 213us/step - loss: 0.4093 - acc: 0.8597 - val_loss: 0.3184 - val_acc: 0.8929\n",
      "Epoch 100/100\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8582  ETA: 10 - ETA: 5s - Epoch 00100: val_loss improved from 0.31596 to 0.31158, saving model to final_model.h5\n",
      "110288/110288 [==============================] - 24s 214us/step - loss: 0.4148 - acc: 0.8582 - val_loss: 0.3116 - val_acc: 0.8939\n",
      "new jersey est parfois calme pendant l'automne de l' automne neigeux il est neigeux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sample 1:\n",
      "il a vu un petit camion noir <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l'automne de l' automne neigeux il est neigeux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model_file = 'final_model.h5'\n",
    "\n",
    "def fit_model(model, x, y):\n",
    "    checkpoint = ModelCheckpoint(filepath=model_file, \n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "    model.fit(x, y, batch_size=1024, \n",
    "                epochs=100, validation_split=0.2, \n",
    "                callbacks=[checkpoint],\n",
    "                verbose=1)\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    x = pad(x, y.shape[1])\n",
    "    if os.path.isfile(model_file):        \n",
    "        print('Continue with last save: {}'.format(model_file))\n",
    "        model = load_model(model_file)\n",
    "    else:\n",
    "        # Train neural network using model_final\n",
    "        model = model_final(\n",
    "            x.shape,\n",
    "            y.shape[1],\n",
    "            len(x_tk.word_index) + 1,\n",
    "            len(y_tk.word_index) + 1)\n",
    "    fit_model(model, x, y)\n",
    "\n",
    "    # Print prediction(s)\n",
    "    print(logits_to_text(model.predict(x[:1])[0], y_tk))\n",
    "\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
